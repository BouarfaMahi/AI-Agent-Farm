

## Readme

In a typical GPU server configuration, multiple GPUs are commonly used, usually ranging from 4 to 8. The Tesla K80 GPUs feature dual GPU dies per card, allowing for increased performance and parallel processing capabilities without the need for an additional GPU server.

By allocating one GPU from the Tesla K80 to each microserver, you can effectively run a single machine learning notebook on each microserver without requiring extra GPUs. This streamlined approach simplifies resource allocation and management, enabling independent operation of each microserver with its dedicated GPU.

The Tesla K80's dual GPU setup not only expands the number of connected microservers available for AI tasks but also optimizes performance and reduces hardware costs. It provides a cost-effective solution for running one machine learning notebook per microserver while efficiently utilizing available GPU resources in a GPU server environment